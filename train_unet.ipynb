{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from modules.unet import UNet\n",
    "from modules.unet import dice_loss, dice_coeff, multiclass_dice_coeff\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from IPython.display import clear_output\n",
    "from torchmetrics import StructuralSimilarityIndexMeasure, PeakSignalNoiseRatio\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.inception import InceptionScore \n",
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n",
    "\n",
    "print('imported')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n",
      "Device name: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "# working device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Selected device: {}'.format(device))\n",
    "print('Device name: {}'.format(torch.cuda.get_device_name(device)))\n",
    "\n",
    "# other cosntants\n",
    "# you can modify from here: and only modify whats possible to modify (for backbones and optimizers)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 40\n",
    "IMG_SIZE = (128, 128)\n",
    "LEARNING_RATE = 0.0001\n",
    "IN_CHANNELS = 1\n",
    "\n",
    "class IdentityDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape: (751, 2, 128, 128) - max: 1.0\n",
      "test_data.shape: (828, 2, 128, 128) - max: 1.0\n",
      "fake_data.shape: (2000, 2, 128, 128) - max: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "# real_data = np.load(os.path.join(DATA_DIR, 'seg_train_data.npy'))\n",
    "# real_data = np.load('./data/brats_preprocessed.npy') # 3D data\n",
    "real_data = np.load('data/brats_preprocessed.npy')\n",
    "train_data, test_data = real_data[60:90], real_data[100:130]  # 30 samples for training, 30 for testing\n",
    "\n",
    "def process_data(real_data):\n",
    "    real_data = real_data.transpose(0, 4, 1, 2, 3)\n",
    "    real_data = real_data.reshape(-1, 2, 128, 128)\n",
    "    # real_data = real_data[:, 0, None, ...]\n",
    "\n",
    "    # drop empty masks\n",
    "    real_data = real_data[real_data[:, 1, ...].sum(axis=(1, 2)) > 0]\n",
    "\n",
    "    # normalize\n",
    "    for idx in range(real_data.shape[0]):\n",
    "        real_data[idx, 0] = (real_data[idx, 0] - real_data[idx, 0].min()) / (real_data[idx, 0].max() - real_data[idx, 0].min())\n",
    "\n",
    "    return real_data\n",
    "\n",
    "train_data = process_data(train_data)\n",
    "test_data = process_data(test_data)\n",
    "\n",
    "fake_data = np.load('data/generated_AHVAE_128_128_ds - hecktor_mask - True.npy')\n",
    "# fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "\n",
    "print('train_data.shape: {} - max: {}'.format(train_data.shape, train_data.max()))\n",
    "print('test_data.shape: {} - max: {}'.format(test_data.shape, test_data.max()))\n",
    "print('fake_data.shape: {} - max: {}'.format(fake_data.shape, fake_data.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images.shape: torch.Size([2679, 1, 128, 128])\n",
      "train_masks.shape: torch.Size([2679, 1, 128, 128])\n",
      "test_images.shape: torch.Size([801, 1, 128, 128])\n",
      "test_masks.shape: torch.Size([801, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# combining fake and real training data\n",
    "n_real, n_fake = -1, 2000\n",
    "train_images = np.concatenate((train_data[:n_real, 0], fake_data[:n_fake, 0]))\n",
    "train_masks = np.concatenate((train_data[:n_real, 1], fake_data[:n_fake, 1]))\n",
    "\n",
    "test_images, test_masks = test_data[:, 0], test_data[:, 1]\n",
    "\n",
    "# shuffling the data\n",
    "rand_idx = np.arange(train_images.__len__())\n",
    "np.random.shuffle(rand_idx)\n",
    "train_images = train_images[rand_idx]\n",
    "train_masks = train_masks[rand_idx]\n",
    "\n",
    "# converting to torch tensors\n",
    "train_images = torch.from_numpy(train_images).type(torch.float32)\n",
    "train_masks = torch.from_numpy(train_masks).type(torch.float32)\n",
    "test_images = torch.from_numpy(test_images).type(torch.float32)\n",
    "test_masks = torch.from_numpy(test_masks).type(torch.float32)\n",
    "\n",
    "# unsqueeze the masks & images\n",
    "train_images = train_images.unsqueeze(1)\n",
    "train_masks = train_masks.unsqueeze(1).round()\n",
    "test_images = test_images.unsqueeze(1)\n",
    "test_masks = test_masks.unsqueeze(1)\n",
    "\n",
    "print('train_images.shape: {}'.format(train_images.shape))\n",
    "print('train_masks.shape: {}'.format(train_masks.shape))\n",
    "print('test_images.shape: {}'.format(test_images.shape))\n",
    "print('test_masks.shape: {}'.format(test_masks.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample images:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_sample = 5\n",
    "rand_idx = np.random.randint(0, train_images.shape[0], n_sample)\n",
    "print('Sample images:')\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(torch.hstack([img for img in train_images[rand_idx, 0]]), cmap='hot')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(torch.hstack([img for img in train_masks[rand_idx, 0]]), cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        model,\n",
    "        dataloader,\n",
    "        n_classes = 1,\n",
    "        epochs: int = 5,\n",
    "        learning_rate: float = 1e-5,\n",
    "        amp: bool = False,\n",
    "        weight_decay: float = 1e-8,\n",
    "        momentum: float = 0.999,\n",
    "        gradient_clipping: float = 2.0,\n",
    "        verbose = True\n",
    "):\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, 1.0, 0.01, epochs)\n",
    "    grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "    criterion = nn.CrossEntropyLoss() if n_classes > 1 else nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    train_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        running_total_loss = 0.0\n",
    "\n",
    "        progress = None\n",
    "        if verbose: progress = tqdm(dataloader, position=0, leave=True)\n",
    "        for i, (images, masks) in enumerate(dataloader):\n",
    "            images, masks = images.to(device, dtype=torch.float32), masks.to(device, dtype=torch.long)\n",
    "\n",
    "            with torch.autocast(device.type, enabled=amp):\n",
    "                masks_pred = model(images)\n",
    "                # if n_classes == 1:\n",
    "                    # loss = criterion(masks_pred.squeeze(1), masks.squeeze(1).float())\n",
    "                loss = dice_loss(torch.sigmoid(masks_pred.squeeze(1)).round(), masks.squeeze(1).float(), multiclass=False)\n",
    "                # else:\n",
    "                #     # loss = criterion(masks_pred, masks)\n",
    "                #     loss = dice_loss(\n",
    "                #         F.softmax(masks_pred, dim=1).float(),\n",
    "                #         F.one_hot(masks, n_classes).permute(0, 3, 1, 2).float(),\n",
    "                #         multiclass=True\n",
    "                #     )\n",
    "\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                grad_scaler.scale(loss).backward()\n",
    "                # torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n",
    "                grad_scaler.step(optimizer)\n",
    "                grad_scaler.update()\n",
    "                # scheduler.step()\n",
    "\n",
    "            running_total_loss += loss.item()\n",
    "\n",
    "            if verbose:\n",
    "                with torch.no_grad():\n",
    "                    progress.update(1)\n",
    "                    progress.set_description('Epoch: {:5}/{} - Actual loss: {:.4f}'.format(\n",
    "                        epoch + 1, EPOCHS, running_total_loss / (i + 1)\n",
    "                    ))\n",
    "\n",
    "        train_loss.append(running_total_loss / len(dataloader))\n",
    "\n",
    "        if verbose: progress.close()\n",
    "        del progress\n",
    "\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return model, train_loss\n",
    "\n",
    "def evaluate_model(model, test_loader, n_classes, device='cuda', verbose=True, **kwargs): \n",
    "    if verbose:\n",
    "        print('--------------------------------------------')\n",
    "        print('Test metrics (on test set)')\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # criterion\n",
    "    criterion = nn.CrossEntropyLoss() if n_classes > 1 else nn.BCEWithLogitsLoss()\n",
    "    fn = dice_coeff if n_classes == 1 else multiclass_dice_coeff\n",
    "\n",
    "    running_dice_score, running_ce_loss = 0.0, 0.0\n",
    "    predicted_masks = []\n",
    "    \n",
    "    # computing predictions\n",
    "    progress = None\n",
    "    if verbose: progress = tqdm(test_loader, position=0, leave=True)\n",
    "    for i, (images, masks) in enumerate(test_loader):\n",
    "        # images, masks = images.to(device, dtype=torch.float32), masks.to(device, dtype=torch.long)\n",
    "        masks_pred = model(images.to(device, dtype=torch.float32))\n",
    "        masks_pred = masks_pred.detach().cpu()\n",
    "        if n_classes == 1:\n",
    "            running_ce_loss += criterion(masks_pred.squeeze(1), masks.squeeze(1).float())\n",
    "            running_dice_score += fn(torch.sigmoid(masks_pred.squeeze(1)), masks.squeeze(1).float(), reduce_batch_first=False)\n",
    "        else:\n",
    "            running_ce_loss += criterion(masks_pred, masks)\n",
    "            running_dice_score += fn(\n",
    "                F.softmax(masks_pred, dim=1).float(),\n",
    "                F.one_hot(masks, n_classes).permute(0, 3, 1, 2).float(),\n",
    "                multiclass=True\n",
    "            )\n",
    "\n",
    "        if verbose: progress.update(1)\n",
    "\n",
    "    # computing main metrics\n",
    "    dice_coefficient = running_dice_score / len(test_loader)\n",
    "    ce_loss = running_ce_loss / len(test_loader)\n",
    "\n",
    "    # adding scores to the history\n",
    "    stats = {}\n",
    "    stats['dice_coeff'] = dice_coefficient\n",
    "    stats['ce_loss'] = ce_loss\n",
    "    stats['dice_loss'] = 1 - dice_coefficient\n",
    "\n",
    "    log = \"\"\"\n",
    "    --------------------------------------------\n",
    "    Total loss (Dice loss + CE loss): {:.3f} ↓\n",
    "    Dice loss: {:.3f} ↓\n",
    "    CE loss: {:.3f} ↓\n",
    "    Dice coefficient: {:.3f} ↑\n",
    "    --------------------------------------------\n",
    "    \"\"\".format(\n",
    "        1 - dice_coefficient + ce_loss, 1 - dice_coefficient, ce_loss, dice_coefficient\n",
    "    )\n",
    "\n",
    "    if verbose: print(log)\n",
    "\n",
    "    return stats, predicted_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess(n_real, n_fake, n_iters, amp, epochs, factor=None, nd=3):\n",
    "    # combining fake and real training data\n",
    "    n_real, n_fake = n_real, n_fake\n",
    "    train_images = np.concatenate((train_data[:n_real, 0], fake_data[:n_fake, 0]))\n",
    "    train_masks = np.concatenate((train_data[:n_real, 1], fake_data[:n_fake, 1]))\n",
    "\n",
    "    train_images = torch.from_numpy(train_images).type(torch.float32).unsqueeze(1)\n",
    "    train_masks = torch.from_numpy(train_masks).type(torch.float32).unsqueeze(1).round()\n",
    "\n",
    "    test_images, test_masks = test_data[:, 0], test_data[:, 1]\n",
    "    test_images = torch.from_numpy(test_images).type(torch.float32).unsqueeze(1)\n",
    "    test_masks = torch.from_numpy(test_masks).type(torch.float32).unsqueeze(1).round()\n",
    "\n",
    "    if factor is not None:\n",
    "        data = torch.hstack((train_images, train_masks))\n",
    "\n",
    "        train_transforms = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=[-90, 90]),\n",
    "        ])\n",
    "\n",
    "        data = data.repeat(factor, 1, 1, 1)\n",
    "        data = torch.stack([train_transforms(data[i]) for i in range(len(data))])\n",
    "\n",
    "        train_images = data[:, 0, None]\n",
    "        train_masks = data[:, 1, None]\n",
    "\n",
    "\n",
    "    # shuffling the data\n",
    "    rand_idx = np.arange(train_images.__len__())\n",
    "    np.random.shuffle(rand_idx)\n",
    "    train_images = train_images[rand_idx]\n",
    "    train_masks = train_masks[rand_idx]\n",
    "\n",
    "\n",
    "    print('train_images.shape: {}'.format(train_images.shape))\n",
    "    print('train_masks.shape: {}'.format(train_masks.shape))\n",
    "    print('test_images.shape: {}'.format(test_images.shape))\n",
    "    print('test_masks.shape: {}'.format(test_masks.shape))\n",
    "\n",
    "\n",
    "    stats = []\n",
    "    for _ in tqdm(range(n_iters), position=0, leave=True):\n",
    "        train_loader = torch.utils.data.DataLoader(IdentityDataset(train_images, train_masks), batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "        test_loader = torch.utils.data.DataLoader(IdentityDataset(test_images, test_masks), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        model = UNet(n_channels=IN_CHANNELS, n_classes=1).to(device)\n",
    "\n",
    "        model, _ = train_model(\n",
    "            model, train_loader, n_classes=1, epochs=epochs, learning_rate=LEARNING_RATE, amp=amp, verbose=False\n",
    "        )\n",
    "\n",
    "        s, _ = evaluate_model(model, test_loader, n_classes=1, verbose=False)\n",
    "        stats.append(s)\n",
    "    \n",
    "    print('mean {} - std: {}'.format(np.mean([s['dice_coeff'] for s in stats]), np.std([s['dice_coeff'] for s in stats])))\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assess(n_real=-1, n_fake=0, n_iters=10, amp=True, epochs=40, factor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images.shape: torch.Size([717, 1, 128, 128])\n",
      "train_masks.shape: torch.Size([717, 1, 128, 128])\n",
      "test_images.shape: torch.Size([801, 1, 128, 128])\n",
      "test_masks.shape: torch.Size([801, 1, 128, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [10:17<00:00, 61.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 0.5960990190505981 - std: 0.040967244654893875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'dice_coeff': tensor(0.6319),\n",
       "  'ce_loss': tensor(1.0575),\n",
       "  'dice_loss': tensor(0.3681)},\n",
       " {'dice_coeff': tensor(0.5952),\n",
       "  'ce_loss': tensor(0.8437),\n",
       "  'dice_loss': tensor(0.4048)},\n",
       " {'dice_coeff': tensor(0.6082),\n",
       "  'ce_loss': tensor(1.5596),\n",
       "  'dice_loss': tensor(0.3918)},\n",
       " {'dice_coeff': tensor(0.5182),\n",
       "  'ce_loss': tensor(1.6396),\n",
       "  'dice_loss': tensor(0.4818)},\n",
       " {'dice_coeff': tensor(0.6407),\n",
       "  'ce_loss': tensor(1.1620),\n",
       "  'dice_loss': tensor(0.3593)},\n",
       " {'dice_coeff': tensor(0.6189),\n",
       "  'ce_loss': tensor(0.7870),\n",
       "  'dice_loss': tensor(0.3811)},\n",
       " {'dice_coeff': tensor(0.5865),\n",
       "  'ce_loss': tensor(0.9686),\n",
       "  'dice_loss': tensor(0.4135)},\n",
       " {'dice_coeff': tensor(0.5854),\n",
       "  'ce_loss': tensor(1.7697),\n",
       "  'dice_loss': tensor(0.4146)},\n",
       " {'dice_coeff': tensor(0.6448),\n",
       "  'ce_loss': tensor(0.7959),\n",
       "  'dice_loss': tensor(0.3552)},\n",
       " {'dice_coeff': tensor(0.5313),\n",
       "  'ce_loss': tensor(1.6072),\n",
       "  'dice_loss': tensor(0.4687)}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assess(n_real=-1, n_fake=500, n_iters=10, amp=True, epochs=40, factor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images.shape: torch.Size([1434, 1, 128, 128])\n",
      "train_masks.shape: torch.Size([1434, 1, 128, 128])\n",
      "test_images.shape: torch.Size([801, 1, 128, 128])\n",
      "test_masks.shape: torch.Size([801, 1, 128, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [31:27<00:00, 188.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 0.6681049466133118 - std: 0.01928713545203209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'dice_coeff': tensor(0.6813),\n",
       "  'ce_loss': tensor(10.6719),\n",
       "  'dice_loss': tensor(0.3187)},\n",
       " {'dice_coeff': tensor(0.6654),\n",
       "  'ce_loss': tensor(11.8987),\n",
       "  'dice_loss': tensor(0.3346)},\n",
       " {'dice_coeff': tensor(0.6902),\n",
       "  'ce_loss': tensor(9.4398),\n",
       "  'dice_loss': tensor(0.3098)},\n",
       " {'dice_coeff': tensor(0.6633),\n",
       "  'ce_loss': tensor(10.8380),\n",
       "  'dice_loss': tensor(0.3367)},\n",
       " {'dice_coeff': tensor(0.6729),\n",
       "  'ce_loss': tensor(11.3172),\n",
       "  'dice_loss': tensor(0.3271)},\n",
       " {'dice_coeff': tensor(0.6657),\n",
       "  'ce_loss': tensor(13.2769),\n",
       "  'dice_loss': tensor(0.3343)},\n",
       " {'dice_coeff': tensor(0.6650),\n",
       "  'ce_loss': tensor(13.5668),\n",
       "  'dice_loss': tensor(0.3350)},\n",
       " {'dice_coeff': tensor(0.6167),\n",
       "  'ce_loss': tensor(16.5277),\n",
       "  'dice_loss': tensor(0.3833)},\n",
       " {'dice_coeff': tensor(0.6756),\n",
       "  'ce_loss': tensor(15.7124),\n",
       "  'dice_loss': tensor(0.3244)},\n",
       " {'dice_coeff': tensor(0.6849),\n",
       "  'ce_loss': tensor(7.5709),\n",
       "  'dice_loss': tensor(0.3151)}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assess(n_real=-1, n_fake=500, n_iters=10, amp=False, epochs=40, factor=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from modules.unet import UNet\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from modules.unet import dice_loss, dice_coeff, multiclass_dice_coeff\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "class IdentityDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "real_data = np.load('data/second_stage_dataset_192x192_200.npy')\n",
    "real_data[:, 1] = np.where(real_data[:, 1] > 0, 1, 0)\n",
    "\n",
    "\n",
    "train_data, test_data = real_data[0:100], real_data[100:]  # 30 samples for training, 30 for testing\n",
    "\n",
    "train_images = torch.from_numpy(train_data[:, 0, None]).type(torch.float32)\n",
    "train_masks = torch.from_numpy(train_data[:, 1, None]).type(torch.float32).round()\n",
    "\n",
    "test_images, test_masks = test_data[:, 0, None], test_data[:, 1, None]\n",
    "test_images = torch.from_numpy(test_images).type(torch.float32)\n",
    "test_masks = torch.from_numpy(test_masks).type(torch.float32).round()\n",
    "\n",
    "# shuffling the data\n",
    "rand_idx = np.arange(train_images.__len__())\n",
    "np.random.shuffle(rand_idx)\n",
    "train_images = train_images[rand_idx]\n",
    "train_masks = train_masks[rand_idx]\n",
    "\n",
    "print('train_images.shape: {}'.format(train_images.shape))\n",
    "print('train_masks.shape: {}'.format(train_masks.shape))\n",
    "print('test_images.shape: {}'.format(test_images.shape))\n",
    "print('test_masks.shape: {}'.format(test_masks.shape))\n",
    "print('max : {}'.format(train_images.max()))\n",
    "print('min : {}'.format(train_images.min()))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(IdentityDataset(train_images, train_masks), batch_size=2, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(IdentityDataset(test_images, test_masks), batch_size=1, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## Training\n",
    "\n",
    "amp = False\n",
    "model = UNet(n_channels=1, n_classes=1, nd=3).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "train_loss = []\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "\n",
    "    running_total_loss = 0.0\n",
    "\n",
    "    progress = tqdm(train_loader, position=0, leave=True)\n",
    "    for i, (images, masks) in enumerate(train_loader):\n",
    "        images, masks = images.to(device, dtype=torch.float32), masks.to(device, dtype=torch.long)\n",
    "\n",
    "        with torch.autocast(device.type, enabled=amp):\n",
    "            masks_pred = model(images)\n",
    "            loss = dice_loss(torch.sigmoid(masks_pred.squeeze(1)), masks.squeeze(1).float())\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            grad_scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "            grad_scaler.step(optimizer)\n",
    "            grad_scaler.update()\n",
    "\n",
    "        running_total_loss += loss.item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            progress.update(1)\n",
    "            progress.set_description('Epoch: {:5}/{} - Actual loss: {:.4f}'.format(\n",
    "                epoch + 1, EPOCHS, running_total_loss / (i + 1)\n",
    "            ))\n",
    "\n",
    "    train_loss.append(running_total_loss / len(train_loader))\n",
    "\n",
    "    progress.close()\n",
    "    del progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (inc): DoubleConv(\n",
       "    (double_conv): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv3d(1, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Sequential(\n",
       "        (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (down2): Down(\n",
       "    (maxpool_conv): Sequential(\n",
       "      (0): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): Sequential(\n",
       "        (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down3): Down(\n",
       "    (maxpool_conv): Sequential(\n",
       "      (0): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): Sequential(\n",
       "        (0): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down4): Down(\n",
       "    (maxpool_conv): Sequential(\n",
       "      (0): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): Sequential(\n",
       "        (0): Conv3d(512, 1024, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "        (1): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up1): Up(\n",
       "    (up): ConvTranspose3d(1024, 512, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv3d(1024, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Sequential(\n",
       "          (0): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up2): Up(\n",
       "    (up): ConvTranspose3d(512, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Sequential(\n",
       "          (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up3): Up(\n",
       "    (up): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Sequential(\n",
       "          (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (outc): OutConv(\n",
       "    (conv): Conv3d(128, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(n_channels=1, n_classes=1, nd=3).to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:04<00:00,  6.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (Dice loss + CE loss): 1.769 ↓\n",
      "    Dice loss: 0.894 ↓\n",
      "    CE loss: 0.875 ↓\n",
      "    Dice coefficient: 0.106 ↑\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'dice_coeff': tensor(0.1058),\n",
       "  'ce_loss': tensor(0.8751),\n",
       "  'dice_loss': tensor(0.8942)},\n",
       " [])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model, test_loader, n_classes=1, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_samples(real, fake, verbose=True, device='cuda', **kwargs): \n",
    "    if verbose:\n",
    "        print('--------------------------------------------')\n",
    "        print('Test metrics (on test set)')\n",
    "\n",
    "    # recon = []\n",
    "    # # computing predictions\n",
    "    # for i, x in enumerate(tqdm(test_loader, position=0, leave=True)):\n",
    "    #     x = x.to(device, dtype=torch.float32)\n",
    "    #     output = model(x)\n",
    "    #     stats = model.loss_function(output['x'], x, output['mu'], output['logvar'])\n",
    "    #     recon.append(output['x'])\n",
    "    stats = dict()\n",
    "\n",
    "    # # computing main metrics\n",
    "    # elbo = stats['loss'].item()\n",
    "    # recon_loss = stats['recon_loss'].item()\n",
    "    # kld_loss = stats['kld_loss'].item()\n",
    "\n",
    "    # real = torch.vstack([img for img in test_loader]).to(device, dtype=torch.float32)\n",
    "    # recon = torch.vstack([img for img in recon]).to(device, dtype=torch.float32)\n",
    "    # fake = model.sample(n_samples=len(test_loader))\n",
    "\n",
    "    print('Evaluation on {} real/fake/recon images'.format(len(real)))\n",
    "\n",
    "    # computing ssim and psnr from outputs\n",
    "    psnr = PeakSignalNoiseRatio().to(device)\n",
    "    psnr_score = psnr(fake, real)\n",
    "    ssim = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "    ssim_score = ssim(fake, real)\n",
    "    \n",
    "    if real.shape[1] == 1:\n",
    "        real = real.repeat(1, 3, 1, 1)\n",
    "        fake = fake.repeat(1, 3, 1, 1)\n",
    "        # recon = recon.repeat(1, 3, 1, 1)\n",
    "\n",
    "    fid = FrechetInceptionDistance(normalize=True).to(device)\n",
    "    fid.update(real, real=True)\n",
    "    fid.update(fake, real=False)\n",
    "    fid_score = fid.compute()\n",
    "    incep = InceptionScore(normalize=True).to(device)\n",
    "    incep.update(fake)\n",
    "    mean_is_score, std_is_score = incep.compute()\n",
    "    is_score = torch.exp(mean_is_score) / torch.sqrt(std_is_score)\n",
    "    lpips = LearnedPerceptualImagePatchSimilarity(net_type='vgg', normalize=True).to(device)\n",
    "    lpips_score = lpips(real, fake)\n",
    "\n",
    "    # adding scores to the history\n",
    "    stats['psnr'] = psnr_score\n",
    "    stats['ssim'] = ssim_score\n",
    "    stats['fid'] = fid_score\n",
    "    stats['is'] = (mean_is_score, std_is_score, is_score)\n",
    "    stats['lpips'] = lpips_score\n",
    "\n",
    "    log = \"\"\"\n",
    "    --------------------------------------------\n",
    "    Total loss (ELBO): {:.3f} ↓\n",
    "    Mean squared error: {:.3f} ↓\n",
    "    Kullback-Leibler divergence: {:.3f} ↓\n",
    "    PSNR: {:.3f} ↑\n",
    "    SSIM: {:.3f} ↑\n",
    "    FID: {:.3f} ↓\n",
    "    IS: (mean) {:.3f} (std) {:.3f} : {:.3f} ↑\n",
    "    LPIPS: {:.3f} ↓\n",
    "    kwargs: {}\n",
    "    --------------------------------------------\n",
    "    \"\"\".format(\n",
    "        0, 0, 0, psnr_score, ssim_score, fid_score, mean_is_score, \n",
    "        std_is_score, is_score, lpips_score, kwargs\n",
    "    )\n",
    "\n",
    "    if verbose: print(log)\n",
    "\n",
    "    return stats\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HECKTOR dHVAE-HECKTOR (original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 609 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 ↓\n",
      "    Mean squared error: 0.000 ↓\n",
      "    Kullback-Leibler divergence: 0.000 ↓\n",
      "    PSNR: 19.115 ↑\n",
      "    SSIM: 0.600 ↑\n",
      "    FID: 74.428 ↓\n",
      "    IS: (mean) 1.027 (std) 0.020 : 19.854 ↑\n",
      "    LPIPS: 0.310 ↓\n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/dHVAE-HECKTOR (original).npy')\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AHVAE - HECKTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 609 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 ↓\n",
      "    Mean squared error: 0.000 ↓\n",
      "    Kullback-Leibler divergence: 0.000 ↓\n",
      "    PSNR: 19.570 ↑\n",
      "    SSIM: 0.580 ↑\n",
      "    FID: 54.008 ↓\n",
      "    IS: (mean) 1.063 (std) 0.023 : 19.259 ↑\n",
      "    LPIPS: 0.309 ↓\n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/generated_AHVAE_128_128_ds - hecktor_mask - True.npy')\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE (HECKTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 609 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 ↓\n",
      "    Mean squared error: 0.000 ↓\n",
      "    Kullback-Leibler divergence: 0.000 ↓\n",
      "    PSNR: 19.038 ↑\n",
      "    SSIM: 0.567 ↑\n",
      "    FID: 90.208 ↓\n",
      "    IS: (mean) 1.037 (std) 0.024 : 18.132 ↑\n",
      "    LPIPS: 0.338 ↓\n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/generated_VAE_128_128_ds=hecktor, mask=True.npy')\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSGAN (HECKTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 609 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 ↓\n",
      "    Mean squared error: 0.000 ↓\n",
      "    Kullback-Leibler divergence: 0.000 ↓\n",
      "    PSNR: 18.440 ↑\n",
      "    SSIM: 0.488 ↑\n",
      "    FID: 189.753 ↓\n",
      "    IS: (mean) 1.773 (std) 0.126 : 16.555 ↑\n",
      "    LPIPS: 0.459 ↓\n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/generated_LSGAN_128_128_ds - hecktor_mask - True.npy')\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BRATS - AHVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 838 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 ↓\n",
      "    Mean squared error: 0.000 ↓\n",
      "    Kullback-Leibler divergence: 0.000 ↓\n",
      "    PSNR: 15.582 ↑\n",
      "    SSIM: 0.647 ↑\n",
      "    FID: 94.679 ↓\n",
      "    IS: (mean) 1.569 (std) 0.054 : 20.587 ↑\n",
      "    LPIPS: 0.227 ↓\n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/generated_AHVAE_128_128_ds - brats_mask - True.npy')\n",
    "fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brats - VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 838 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 ↓\n",
      "    Mean squared error: 0.000 ↓\n",
      "    Kullback-Leibler divergence: 0.000 ↓\n",
      "    PSNR: 15.181 ↑\n",
      "    SSIM: 0.644 ↑\n",
      "    FID: 106.276 ↓\n",
      "    IS: (mean) 1.730 (std) 0.144 : 14.853 ↑\n",
      "    LPIPS: 0.273 ↓\n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/generated_VAE_128_128_ds=brats, mask=True.npy')\n",
    "fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HVAE BRATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 838 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 ↓\n",
      "    Mean squared error: 0.000 ↓\n",
      "    Kullback-Leibler divergence: 0.000 ↓\n",
      "    PSNR: 15.336 ↑\n",
      "    SSIM: 0.655 ↑\n",
      "    FID: 124.750 ↓\n",
      "    IS: (mean) 1.312 (std) 0.057 : 15.542 ↑\n",
      "    LPIPS: 0.248 ↓\n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/generated_HVAE_128_128_ds - brats, mask - True, dt - 0.1.npy')\n",
    "fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSGAN BRATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 500 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 ↓\n",
      "    Mean squared error: 0.000 ↓\n",
      "    Kullback-Leibler divergence: 0.000 ↓\n",
      "    PSNR: 14.579 ↑\n",
      "    SSIM: 0.442 ↑\n",
      "    FID: 158.289 ↓\n",
      "    IS: (mean) 1.856 (std) 0.066 : 24.832 ↑\n",
      "    LPIPS: 0.473 ↓\n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_images = np.load('data/generated_LSGAN_128_128_ds - brats_mask - True.npy')\n",
    "fake_images = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images[:500].to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRATS - dHVAE old training with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 838 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 ↓\n",
      "    Mean squared error: 0.000 ↓\n",
      "    Kullback-Leibler divergence: 0.000 ↓\n",
      "    PSNR: 15.614 ↑\n",
      "    SSIM: 0.647 ↑\n",
      "    FID: 92.488 ↓\n",
      "    IS: (mean) 1.582 (std) 0.052 : 21.427 ↑\n",
      "    LPIPS: 0.227 ↓\n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/dHVAE-BRATS (original).npy')\n",
    "fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dHVAE - BRATS (0.01) = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 838 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 ↓\n",
      "    Mean squared error: 0.000 ↓\n",
      "    Kullback-Leibler divergence: 0.000 ↓\n",
      "    PSNR: 13.795 ↑\n",
      "    SSIM: 0.578 ↑\n",
      "    FID: 123.964 ↓\n",
      "    IS: (mean) 1.719 (std) 0.086 : 19.076 ↑\n",
      "    LPIPS: 0.283 ↓\n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/dHVAE-BRATS (0.05).npy')\n",
    "fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRATS - dHVAE (0.001) = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 838 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 ↓\n",
      "    Mean squared error: 0.000 ↓\n",
      "    Kullback-Leibler divergence: 0.000 ↓\n",
      "    PSNR: 15.409 ↑\n",
      "    SSIM: 0.632 ↑\n",
      "    FID: 126.503 ↓\n",
      "    IS: (mean) 1.707 (std) 0.055 : 23.470 ↑\n",
      "    LPIPS: 0.233 ↓\n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/dHVAE-BRATS (0.05).npy')\n",
    "# fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dHVAE - BRATS (0.1) = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 838 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 ↓\n",
      "    Mean squared error: 0.000 ↓\n",
      "    Kullback-Leibler divergence: 0.000 ↓\n",
      "    PSNR: 15.208 ↑\n",
      "    SSIM: 0.633 ↑\n",
      "    FID: 147.453 ↓\n",
      "    IS: (mean) 1.671 (std) 0.049 : 24.154 ↑\n",
      "    LPIPS: 0.235 ↓\n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/dHVAE-BRATS (0.1).npy')\n",
    "# fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dHVAE - BRATS (0.01) Fethi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 838 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 ↓\n",
      "    Mean squared error: 0.000 ↓\n",
      "    Kullback-Leibler divergence: 0.000 ↓\n",
      "    PSNR: 15.622 ↑\n",
      "    SSIM: 0.627 ↑\n",
      "    FID: 88.299 ↓\n",
      "    IS: (mean) 1.817 (std) 0.075 : 22.525 ↑\n",
      "    LPIPS: 0.229 ↓\n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/dHVAE-BRATS (0.01) Fethi.npy')\n",
    "# fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HVAE + Percep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 838 real/fake/recon images\n",
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 ↓\n",
      "    Mean squared error: 0.000 ↓\n",
      "    Kullback-Leibler divergence: 0.000 ↓\n",
      "    PSNR: 14.923 ↑\n",
      "    SSIM: 0.637 ↑\n",
      "    FID: 104.767 ↓\n",
      "    IS: (mean) 1.600 (std) 0.052 : 21.715 ↑\n",
      "    LPIPS: 0.225 ↓\n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/HVAE-Percep-BRATS.npy')\n",
    "# fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HVAE + Disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 838 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 ↓\n",
      "    Mean squared error: 0.000 ↓\n",
      "    Kullback-Leibler divergence: 0.000 ↓\n",
      "    PSNR: 15.102 ↑\n",
      "    SSIM: 0.633 ↑\n",
      "    FID: 147.954 ↓\n",
      "    IS: (mean) 1.681 (std) 0.085 : 18.456 ↑\n",
      "    LPIPS: 0.237 ↓\n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/HVAE-Disc-Brats.npy')\n",
    "# fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fae847446f25d1d5cccaa632528b81cb53ce4d6408a7df79225531d1adf33a86"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
